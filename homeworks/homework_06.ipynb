{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Poisson regression is a Generalized Linear Model, used to model count data. It takes the form\n",
    "\n",
    "$$\\mathbb{E}(\\mu|x)=\\exp(w_1\\,x_1+\\ldots+w_k\\,x_k+b),$$\n",
    "\n",
    "where the observed counts $y$ are drawn from a Poisson distribution on the expected counts: \n",
    "\n",
    "$$y_i \\sim \\text{Poisson}(\\mu_i).$$\n",
    "\n",
    "1. Download and import Load the smoking dataset from: [https://data.princeton.edu/wws509/datasets/#smoking](https://data.princeton.edu/wws509/datasets/#smoking). Then perform a train-test split on the data;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.infer import Predictive\n",
    "\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "\n",
    "pyro.set_rng_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age_Group</th>\n",
       "      <th>Smoking_Status</th>\n",
       "      <th>Population</th>\n",
       "      <th>Deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>656</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>359</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>249</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>632</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1067</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age_Group  Smoking_Status  Population  Deaths\n",
       "0          1               1         656      18\n",
       "1          2               1         359      22\n",
       "2          3               1         249      19\n",
       "3          4               1         632      55\n",
       "4          5               1        1067     117"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"https://data.princeton.edu/wws509/datasets/smoking.raw\",sep=\"\\t\",header=None)\n",
    "data.columns = [\"Age_Group\",\"Smoking_Status\",\"Population\",\"Deaths\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* age at the start of follow-up: in five-year age groups coded 1 to 9 for 40-44, 45-49, 50-54, 55-59, 60-64, 65-69, 70-74, 75-79, 80+.\n",
    "* smoking status: coded 1 = never smoked, 2 = smoked cigars or pipe only, 3 = smoked cigarettes and cigar or pipe, and 4 = smoked cigarettes only,\n",
    "* population: number of male pensioners followed, and\n",
    "* deaths: number of deaths in a six-year period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36 entries, 0 to 35\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype\n",
      "---  ------          --------------  -----\n",
      " 0   Age_Group       36 non-null     int64\n",
      " 1   Smoking_Status  36 non-null     int64\n",
      " 2   Population      36 non-null     int64\n",
      " 3   Deaths          36 non-null     int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 1.2 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform a train-test split on the data:\n",
    "- **train data** - 80% of the observations will be used to perfom inference on our model\n",
    "- **test data** - the remaining 20% will be used for testing the correctness of posterior predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the population since it dominates the predictors  * X\n",
    "scaler = MinMaxScaler()\n",
    "data[\"Population\"] = scaler.fit_transform(pd.DataFrame(data[\"Population\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths = torch.tensor(data[\"Deaths\"].values, dtype=torch.float)\n",
    "predictors = torch.stack([torch.tensor(data[column].values,dtype=torch.float) \n",
    "                            for column in data.columns if column != \"Deaths\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = torch.Size([28, 3]) \n",
      "y_train.shape = torch.Size([28])\n",
      "\n",
      "X_test.shape = torch.Size([8, 3]) \n",
      "y_test.shape = torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(predictors, deaths, test_size=0.20, \n",
    "                                                    random_state=42,shuffle=True)\n",
    "\n",
    "print(\"X_train.shape =\", X_train.shape,\"\\ny_train.shape =\", y_train.shape)\n",
    "print(\"\\nX_test.shape =\", X_test.shape,\"\\ny_test.shape =\", y_test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "k = int(0.8 * len(data))\n",
    "X_train, y_train = predictors[:k], deaths[:k]\n",
    "X_test, y_test = predictors[k:], deaths[k:]\n",
    "\n",
    "print(\"x_train.shape =\", X_train.shape,\"\\ny_train.shape =\", y_train.shape)\n",
    "print(\"\\nx_test.shape =\", X_test.shape,\"\\ny_test.shape =\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fit a Poisson bayesian regression model using the number of deaths as the response variable and the other columns as the explanatory variables;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable in our model is the number of deaths and we wish to infer the parameters corresponding to the following predictors\n",
    "\n",
    "$$\n",
    "\\text{Deaths}=\\text{exp}(w_0\\cdot\\text{Age_Group}+w_1\\cdot\\text{Smoking_Status}+w_2\\cdot\\text{Population}+b) + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set a normal prior on $w$, a Log-Normal on the bias term $b$ and a uniformly distributed std for the gaussian noise on $\\hat{y}$\n",
    "\n",
    "\\begin{align*}\n",
    "w&\\sim\\mathcal{N}(0,1)\\\\\n",
    "b&\\sim\\text{LogNormal}(0,1)\\\\\n",
    "\\hat{\\mu}&= \\text{exp}(w x+ b) \\\\\n",
    "y &\\sim \\text{Poisson}(\\hat{\\mu}).\n",
    "\\end{align*}\n",
    "\n",
    "Then we define the family of posterior distributions, by setting a Gamma distribution on $w$ and a Log-Normal on $b$, and run SVI inference on $(x,y)$ data.\n",
    "\n",
    "Notice the prior distribution on the bias term makes this regression problem analytically intractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**will be read**\n",
    "\n",
    "https://towardsdatascience.com/an-illustrated-guide-to-the-poisson-regression-model-50cccba15958\n",
    "\n",
    "https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Poisson_Regression.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Poisson ile ilgili okuma yapılacak, tanım aralıklarına bakılacak**\n",
    "\n",
    "**w  için negatif değer bulunmaması gerekiyor ona bakılacak, muhati nasıl pozitif yaparım** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 : loss = 70.29648329956191\n",
      "Step 100 : loss = 147.51487459880966\n",
      "Step 200 : loss = 762.3052494632346\n",
      "Step 300 : loss = 1110.694842006479\n",
      "Step 400 : loss = 129166.11681348724\n",
      "Step 500 : loss = 27257.90517469389\n",
      "Step 600 : loss = 945.941014362233\n",
      "Step 700 : loss = 646.1101501371179\n",
      "Step 800 : loss = 4804417.224761373\n",
      "Step 900 : loss = 1449166.9001140913\n",
      "Step 1000 : loss = 154.2531350031495\n",
      "Step 1100 : loss = 646.3478335163423\n",
      "Step 1200 : loss = 1969.7512530982494\n",
      "Step 1300 : loss = 46173742.12855301\n",
      "Step 1400 : loss = 400650.3777360767\n",
      "Step 1500 : loss = 237.35848464603936\n",
      "Step 1600 : loss = 673.6445791700056\n",
      "Step 1700 : loss = 126.5137655287981\n",
      "Step 1800 : loss = 6307899.792603889\n",
      "Step 1900 : loss = 13187.547154643706\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "def death_model(predictors, deaths):\n",
    "    \n",
    "    n_observations, n_predictors = predictors.shape\n",
    "    \n",
    "    # sample weights\n",
    "    w = pyro.sample(\"w\", dist.Normal(torch.zeros(n_predictors), \n",
    "                                        torch.ones(n_predictors)))\n",
    "    b = pyro.sample(\"b\", dist.LogNormal(torch.zeros(1), torch.ones(1)))\n",
    "    \n",
    "    mu_hat = torch.exp((w*predictors).sum(dim=1) + b)\n",
    "    \n",
    "    # condition on the observations\n",
    "    with pyro.plate(\"deaths\", len(deaths)):\n",
    "        pyro.sample(\"obs\", dist.Poisson(mu_hat), obs=deaths)\n",
    "        \n",
    "def death_guide(predictors, deaths=None):\n",
    "    \n",
    "    n_observations, n_predictors = predictors.shape\n",
    "        \n",
    "    w_loc = pyro.param(\"w_loc\", torch.rand(n_predictors), constraint=constraints.positive)\n",
    "    w_scale = pyro.param(\"w_scale\", torch.rand(n_predictors), \n",
    "                         constraint=constraints.positive)\n",
    "    \n",
    "    w = pyro.sample(\"w\", dist.Gamma(w_loc, w_scale))\n",
    "    \n",
    "    b_loc = pyro.param(\"b_loc\", torch.rand(1))\n",
    "    b_scale = pyro.param(\"b_scale\", torch.rand(1), constraint=constraints.positive)\n",
    "    \n",
    "    b = pyro.sample(\"b\", dist.LogNormal(b_loc, b_scale))\n",
    "    \n",
    "\n",
    "\n",
    "#smokeguide = pyro.infer.autoguide.AutoMultivariateNormal(death_model)\n",
    "    \n",
    "death_svi = SVI(model=death_model, guide=death_guide, \n",
    "              optim=optim.ClippedAdam({'lr' : 0.01}), \n",
    "              loss=Trace_ELBO())\n",
    "\n",
    "for step in range(2000):\n",
    "    loss = death_svi.step(X_train, y_train)/len(X_train)\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step} : loss = {loss}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "param = list(pyro.get_param_store().keys())\n",
    "print(\"Inferred params:\", param, end=\"\\n\\n\")\n",
    "\n",
    "inferred = pyro.get_param_store()[param[0]]\n",
    "\n",
    "inferred_w = inferred[0:3]\n",
    "inferred_b = inferred[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred params: ['w_loc', 'w_scale', 'b_loc', 'b_scale']\n",
      "\n",
      "w_0 = 0.58638030\n",
      "w_1 = 0.59420520\n",
      "w_2 = 0.81937784\n",
      "b = -0.05656544\n"
     ]
    }
   ],
   "source": [
    "print(\"Inferred params:\", list(pyro.get_param_store().keys()), end=\"\\n\\n\")\n",
    "\n",
    "# w_i and b posterior mean\n",
    "inferred_w = pyro.get_param_store()[\"w_loc\"]\n",
    "inferred_b = pyro.get_param_store()[\"b_loc\"]\n",
    "\n",
    "for i,w in enumerate(inferred_w):\n",
    "    print(f\"w_{i} = {w.item():.8f}\")\n",
    "print(f\"b = {inferred_b.item():.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Posterior predictive distribution**\n",
    "\n",
    "We can use the `Predictive` utility class, corresponding to the posterior predictive distribution, to evaluate our model on test data. Here we compute some summary statistics (mean, std and qualtiles) on $100$ samples from the posterior predictive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled parameter = w\n",
      "\n",
      "       mean       std        5%       50%       95%\n",
      "0  0.397923  0.489770  0.005756  0.207282  1.571548\n",
      "1  0.840930  0.985975  0.002140  0.376907  3.210741\n",
      "2  1.836145  2.281337  0.090406  0.941019  6.174343\n",
      "\n",
      "Sampled parameter = b\n",
      "\n",
      "       mean      std        5%       50%       95%\n",
      "0  0.946605  0.06728  0.843839  0.945172  1.065237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print latent params quantile information\n",
    "def summary(samples):\n",
    "    stats = {}\n",
    "    for par_name, values in samples.items():\n",
    "        marginal = pd.DataFrame(values)\n",
    "        percentiles=[.05, 0.5, 0.95]\n",
    "        describe = marginal.describe(percentiles).transpose()\n",
    "        stats[par_name] = describe[[\"mean\", \"std\", \"5%\", \"50%\", \"95%\"]]\n",
    "    return stats\n",
    "\n",
    "# define the posterior predictive\n",
    "predictive = Predictive(model=death_model, guide=death_guide, num_samples=100,\n",
    "                        return_sites=(\"w\",\"b\"))\n",
    "\n",
    "# get posterior samples on test data\n",
    "svi_samples = {k: v.detach().numpy() for k, v in predictive(X_test, y_test).items()}\n",
    "\n",
    "# show summary statistics\n",
    "for key, value in summary(svi_samples).items():\n",
    "    print(f\"Sampled parameter = {key}\\n\\n{value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Evaluate the regression fit on test data using MAE and MSE error metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most known metrics for comparing different regression models are the **Mean Absolute Error** (MAE)\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|$$\n",
    "\n",
    "and the **Mean Squared Error** (MSE)\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2,$$\n",
    "\n",
    "where $n$ is the number of observations, $y$ are the true values `y_test` and $\\hat{y}$ are the predicted values `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE = 562.3972778320312\n",
      "MSE = 722826.5625\n"
     ]
    }
   ],
   "source": [
    "# compute predictions using the inferred paramters\n",
    "y_pred = torch.exp((inferred_w * X_test).sum(1)) + inferred_b\n",
    "\n",
    "print(\"MAE =\", torch.nn.L1Loss()(y_test, y_pred).item())\n",
    "print(\"MSE =\", torch.nn.MSELoss()(y_test, y_pred).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "The Iris dataset contains petal and sepal length and width for three different types of Iris flowers: Setosa, Versicolour, and Virginica.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. Evaluate your bayesian classifier on test data: compute the overall test accuracy and class-wise accuracy for the three different flower categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the Iris dataset from `sklearn`:\n",
    "```\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "```\n",
    "and perform a train-test split on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset normalization and OneHotEncoding\n",
    "iris_data = (iris.data - np.min(iris.data))/(np.max(iris.data)-np.min(iris.data))\n",
    "\n",
    "# Each class will be 0 before vs rest will be 1 \n",
    "\n",
    "# 0 vs rest dataset , just make 2 -> 1\n",
    "zerovsrest = np.where((iris.target == 2),1,iris.target)\n",
    "# 1 vs rest dataset , make 1 -> 0 and 0,2 -> 1\n",
    "onevsrest = np.where((iris.target == 2),-1,iris.target)\n",
    "onevsrest = np.where((onevsrest == 0),-1,onevsrest)\n",
    "onevsrest = np.where((onevsrest == 1),0,onevsrest)\n",
    "onevsrest = np.where((onevsrest == -1),1,onevsrest)\n",
    "\n",
    "# 2 vs rest dataset, make 0,1 -> 1 and 2 -> 0\n",
    "twovsrest = np.where((iris.target == 0),1,iris.target)\n",
    "twovsrest = np.where((twovsrest == 2),0,twovsrest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = torch.tensor(iris_data,dtype=torch.double)\n",
    "iris_target = torch.tensor(iris.target,dtype=torch.double)\n",
    "zerovsrest = torch.tensor(zerovsrest,dtype=torch.double)\n",
    "onevsrest = torch.tensor(onevsrest,dtype=torch.double)\n",
    "twovsrest = torch.tensor(twovsrest,dtype=torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fit a multinomial bayesian logistic regression model on the four predictors petal length/width and sepal length/width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and guide functions\n",
    "def log_reg_model(x, y):\n",
    "    n_observations, n_predictors = x.shape\n",
    "    \n",
    "    w = pyro.sample(\"w\", dist.Normal(torch.zeros(n_predictors), torch.ones(n_predictors)))\n",
    "    b = pyro.sample(\"b\", dist.Normal(0.,1.))\n",
    "    \n",
    "    # non-linearity\n",
    "    yhat = torch.sigmoid((w*x).sum(dim=1) + b)\n",
    "    \n",
    "    with pyro.plate(\"data\", n_observations):\n",
    "        # sampling 0-1 labels from Bernoulli distribution\n",
    "        y = pyro.sample(\"y\", dist.Bernoulli(yhat), obs=y)\n",
    "        \n",
    "def log_reg_guide(x, y=None):\n",
    "    \n",
    "    n_observations, n_predictors = x.shape\n",
    "    \n",
    "    w_loc = pyro.param(\"w_loc\", torch.rand(n_predictors))\n",
    "    w_scale = pyro.param(\"w_scale\", torch.rand(n_predictors), \n",
    "                         constraint=constraints.positive)\n",
    "    w = pyro.sample(\"w\", dist.Normal(w_loc, w_scale))\n",
    "    \n",
    "    b_loc = pyro.param(\"b_loc\", torch.rand(1))\n",
    "    b_scale = pyro.param(\"b_scale\", torch.rand(1), \n",
    "                         constraint=constraints.positive)\n",
    "    b = pyro.sample(\"b\", dist.Normal(b_loc, b_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_infer_parameters(X_train ,y_train):\n",
    "    # delete previously inferred params from pyro.param_store()\n",
    "    pyro.clear_param_store()\n",
    "    \n",
    "    log_reg_svi = SVI(model=log_reg_model, guide=log_reg_guide, \n",
    "              optim=optim.ClippedAdam({'lr' : 0.0002}), \n",
    "              loss=Trace_ELBO()) \n",
    "\n",
    "    losses = []\n",
    "    for step in range(10000):\n",
    "        loss = log_reg_svi.step(X_train, y_train)/len(X_train)\n",
    "        losses.append(loss)\n",
    "        if step % 1000 == 0:\n",
    "            print(f\"Step {step} : loss = {loss}\")\n",
    "            \n",
    "    w = pyro.get_param_store()[\"w_loc\"]\n",
    "    b = pyro.get_param_store()[\"b_loc\"]\n",
    "    \n",
    "    return w,b\n",
    "\n",
    "def predict_class(w,b,x):\n",
    "    out = torch.sigmoid((w * x).sum(dim=1) + b)\n",
    "    print(out)\n",
    "    print(out>0.5)\n",
    "    return (out > 0.5)\n",
    "\n",
    "def return_prob(w,b,x):\n",
    "    out = torch.sigmoid((w * x).sum(dim=1) + b)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'figsize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-e8e5dbbf19c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ELBO loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'figsize' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "ax.plot(losses)\n",
    "ax.set_title(\"ELBO loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 : loss = 0.755019464279618\n",
      "Step 1000 : loss = 0.7127734735863194\n",
      "Step 2000 : loss = 0.6822780439594306\n",
      "Step 3000 : loss = 0.7716861839703602\n",
      "Step 4000 : loss = 0.5577056783150035\n",
      "Step 5000 : loss = 0.6744040640989016\n",
      "Step 6000 : loss = 0.5955440425212685\n",
      "Step 7000 : loss = 0.5436122392058109\n",
      "Step 8000 : loss = 0.40504829698803096\n",
      "Step 9000 : loss = 0.5070481985581807\n",
      "tensor([0.7545, 0.5470, 0.8737, 0.7596, 0.7715, 0.5457, 0.7099, 0.8179, 0.7670,\n",
      "        0.7209, 0.8034, 0.5216, 0.5217, 0.5264, 0.5320, 0.7699, 0.8346, 0.7165,\n",
      "        0.7492, 0.8296, 0.5354, 0.7877, 0.5480, 0.8259, 0.8450, 0.8209, 0.8243,\n",
      "        0.8409, 0.5345, 0.5374], dtype=torch.float64,\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True])\n",
      "test accuracy = 66.67%\n"
     ]
    }
   ],
   "source": [
    "# split all the dataset , 0->0  vs 1,2 -> 1 ->\n",
    "X_train_zero, X_test_zero, y_train_zero, y_test_zero = train_test_split(iris_data, zerovsrest, test_size=0.20, \n",
    "                                                    random_state=42)\n",
    "w_zero , b_zero = learn_infer_parameters(X_train_zero, y_train_zero)\n",
    "\n",
    "correct_predictions_0 = (predict_class(w_zero, b_zero, X_test_zero) == y_test_zero).sum().item()\n",
    "out_zero = return_prob(w_zero, b_zero, X_test_zero)\n",
    "print(f\"test accuracy = {correct_predictions_0/len(X_test_zero)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 : loss = 0.7622346981497066\n",
      "Step 1000 : loss = 0.7583319368874285\n",
      "Step 2000 : loss = 0.8202136347158617\n",
      "Step 3000 : loss = 0.6931911362867472\n",
      "Step 4000 : loss = 0.7069921968572507\n",
      "Step 5000 : loss = 0.7164076395192318\n",
      "Step 6000 : loss = 0.6647996456930353\n",
      "Step 7000 : loss = 0.7017418486098015\n",
      "Step 8000 : loss = 0.6846783682758677\n",
      "Step 9000 : loss = 0.6635610847387302\n",
      "tensor([0.6469, 0.6742, 0.6491, 0.6457, 0.6528, 0.6616, 0.6436, 0.6529, 0.6320,\n",
      "        0.6419, 0.6532, 0.6487, 0.6666, 0.6520, 0.6680, 0.6569, 0.6466, 0.6360,\n",
      "        0.6418, 0.6411, 0.6511, 0.6462, 0.6572, 0.6419, 0.6804, 0.6484, 0.6409,\n",
      "        0.6534, 0.6470, 0.6500], dtype=torch.float64,\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True])\n",
      "test accuracy = 70.00%\n"
     ]
    }
   ],
   "source": [
    "# split all the dataset, 1 -> 0 , 0,2 ->1\n",
    "X_train_one, X_test_one, y_train_one, y_test_one = train_test_split(iris_data, onevsrest, test_size=0.20, \n",
    "                                                    random_state=42)\n",
    "\n",
    "w_one , b_one = learn_infer_parameters(X_train_one, y_train_one)\n",
    "correct_predictions_1 = (predict_class(w_one, b_one, X_test_one) == y_test_one).sum().item()\n",
    "out_one = return_prob(w_one, b_one, X_test_one)\n",
    "print(f\"test accuracy = {correct_predictions_1/len(X_test_one)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 : loss = 0.7802328453812867\n",
      "Step 1000 : loss = 0.6617454357905336\n",
      "Step 2000 : loss = 0.6906303834054249\n",
      "Step 3000 : loss = 0.7056654246237097\n",
      "Step 4000 : loss = 0.6625433119263082\n",
      "Step 5000 : loss = 0.6278532156133321\n",
      "Step 6000 : loss = 0.6447132386968544\n",
      "Step 7000 : loss = 0.776075068137491\n",
      "Step 8000 : loss = 0.5476864286437073\n",
      "Step 9000 : loss = 0.5873024768809179\n",
      "tensor([0.6151, 0.7435, 0.5197, 0.6141, 0.6110, 0.7348, 0.6433, 0.5840, 0.5953,\n",
      "        0.6334, 0.5933, 0.7318, 0.7473, 0.7323, 0.7448, 0.6187, 0.5600, 0.6294,\n",
      "        0.6152, 0.5594, 0.7286, 0.5967, 0.7301, 0.5626, 0.5817, 0.5766, 0.5591,\n",
      "        0.5619, 0.7268, 0.7268], dtype=torch.float64,\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True])\n",
      "test accuracy = 63.33%\n"
     ]
    }
   ],
   "source": [
    "# split all the dataset 2->0 , 0,1 -> 1\n",
    "X_train_two, X_test_two, y_train_two, y_test_two = train_test_split(iris_data, twovsrest, test_size=0.20, \n",
    "                                                    random_state=42)\n",
    "\n",
    "w_two , b_two = learn_infer_parameters(X_train_two, y_train_two)\n",
    "correct_predictions_2 = (predict_class(w_two, b_two, X_test_two) == y_test_two).sum().item()\n",
    "out_two = return_prob(w_two, b_two, X_test_two)\n",
    "print(f\"test accuracy = {correct_predictions_2/len(X_test_two)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 0., 2., 1., 1., 0., 1., 2., 1., 1., 2., 0., 0., 0., 0., 1., 2., 1.,\n",
       "         1., 2., 0., 2., 0., 2., 2., 2., 2., 2., 0., 0.], dtype=torch.float64),\n",
       " tensor([1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64),\n",
       " tensor([1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.], dtype=torch.float64))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_target, test_size=0.20, \n",
    "                                                    random_state=42)\n",
    "y_test,y_test_zero, y_test_one ,y_test_two\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7545, 0.6469, 0.6151],\n",
       "        [0.5470, 0.6742, 0.7435],\n",
       "        [0.8737, 0.6491, 0.5197],\n",
       "        [0.7596, 0.6457, 0.6141],\n",
       "        [0.7715, 0.6528, 0.6110],\n",
       "        [0.5457, 0.6616, 0.7348],\n",
       "        [0.7099, 0.6436, 0.6433],\n",
       "        [0.8179, 0.6529, 0.5840],\n",
       "        [0.7670, 0.6320, 0.5953],\n",
       "        [0.7209, 0.6419, 0.6334],\n",
       "        [0.8034, 0.6532, 0.5933],\n",
       "        [0.5216, 0.6487, 0.7318],\n",
       "        [0.5217, 0.6666, 0.7473],\n",
       "        [0.5264, 0.6520, 0.7323],\n",
       "        [0.5320, 0.6680, 0.7448],\n",
       "        [0.7699, 0.6569, 0.6187],\n",
       "        [0.8346, 0.6466, 0.5600],\n",
       "        [0.7165, 0.6360, 0.6294],\n",
       "        [0.7492, 0.6418, 0.6152],\n",
       "        [0.8296, 0.6411, 0.5594],\n",
       "        [0.5354, 0.6511, 0.7286],\n",
       "        [0.7877, 0.6462, 0.5967],\n",
       "        [0.5480, 0.6572, 0.7301],\n",
       "        [0.8259, 0.6419, 0.5626],\n",
       "        [0.8450, 0.6804, 0.5817],\n",
       "        [0.8209, 0.6484, 0.5766],\n",
       "        [0.8243, 0.6409, 0.5591],\n",
       "        [0.8409, 0.6534, 0.5619],\n",
       "        [0.5345, 0.6470, 0.7268],\n",
       "        [0.5374, 0.6500, 0.7268]], dtype=torch.float64,\n",
       "       grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_final = torch.stack((out_zero,out_one,out_two),dim=1)\n",
    "out_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True,  True, False, False,  True, False,  True, False, False,\n",
       "         True,  True,  True,  True,  True, False,  True, False, False,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmin(out_final,dim=1) == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 2, 0, 2,\n",
       "        2, 2, 2, 2, 0, 0], grad_fn=<NotImplemented>)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmin(out_final,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy = 70.00%\n"
     ]
    }
   ],
   "source": [
    "correct_predictions_final = (torch.argmin(out_final,dim=1) == y_test).sum().item()\n",
    "print(f\"test accuracy = {correct_predictions_final/len(X_test)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Bayesian logistic regression function returns a probability score between 0 and 1 on each input point $x$, which is the output of the sigmoid function. \n",
    "\n",
    "In order to map this value to a discrete class, `predict_class()` uses a **threshold value** of 0.5 to decide whether $x$ belongs to class $0$ or $1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, we can compute the **test accuracy** of our model, which is the percentage of correct predictions on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy = 0.00%\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = (predict_class(X_test) == y_test).sum().item()\n",
    "\n",
    "print(f\"test accuracy = {correct_predictions/len(X_test)*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
