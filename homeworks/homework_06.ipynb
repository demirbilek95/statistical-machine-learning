{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Poisson regression is a Generalized Linear Model, used to model count data. It takes the form\n",
    "\n",
    "$$\\mathbb{E}(\\mu|x)=\\exp(w_1\\,x_1+\\ldots+w_k\\,x_k+b),$$\n",
    "\n",
    "where the observed counts $y$ are drawn from a Poisson distribution on the expected counts: \n",
    "\n",
    "$$y_i \\sim \\text{Poisson}(\\mu_i).$$\n",
    "\n",
    "1. Download and import Load the smoking dataset from: [https://data.princeton.edu/wws509/datasets/#smoking](https://data.princeton.edu/wws509/datasets/#smoking). Then perform a train-test split on the data;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "pyro.clear_param_store()\n",
    "\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.infer import Predictive\n",
    "\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "\n",
    "pyro.set_rng_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age_Group</th>\n",
       "      <th>Smoking_Status</th>\n",
       "      <th>Population</th>\n",
       "      <th>Deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40-44</td>\n",
       "      <td>no</td>\n",
       "      <td>656</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45-59</td>\n",
       "      <td>no</td>\n",
       "      <td>359</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50-54</td>\n",
       "      <td>no</td>\n",
       "      <td>249</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55-59</td>\n",
       "      <td>no</td>\n",
       "      <td>632</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60-64</td>\n",
       "      <td>no</td>\n",
       "      <td>1067</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Age_Group Smoking_Status  Population  Deaths\n",
       "1     40-44             no         656      18\n",
       "2     45-59             no         359      22\n",
       "3     50-54             no         249      19\n",
       "4     55-59             no         632      55\n",
       "5     60-64             no        1067     117"
      ]
     },
     "execution_count": 973,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading data\n",
    "data = pd.read_csv(\"https://data.princeton.edu/wws509/datasets/smoking.dat\",sep=\"\\s+\")\n",
    "data.columns = [\"Age_Group\",\"Smoking_Status\",\"Population\",\"Deaths\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used one hot encoding for categorical variables because there is no order in categorical variables and using label encoder may manipulate the result of regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 36 entries, 1 to 36\n",
      "Data columns (total 15 columns):\n",
      " #   Column                         Non-Null Count  Dtype\n",
      "---  ------                         --------------  -----\n",
      " 0   Population                     36 non-null     int64\n",
      " 1   Deaths                         36 non-null     int64\n",
      " 2   Age_Group_40-44                36 non-null     uint8\n",
      " 3   Age_Group_45-59                36 non-null     uint8\n",
      " 4   Age_Group_50-54                36 non-null     uint8\n",
      " 5   Age_Group_55-59                36 non-null     uint8\n",
      " 6   Age_Group_60-64                36 non-null     uint8\n",
      " 7   Age_Group_65-69                36 non-null     uint8\n",
      " 8   Age_Group_70-74                36 non-null     uint8\n",
      " 9   Age_Group_75-79                36 non-null     uint8\n",
      " 10  Age_Group_80+                  36 non-null     uint8\n",
      " 11  Smoking_Status_cigarPipeOnly   36 non-null     uint8\n",
      " 12  Smoking_Status_cigarretteOnly  36 non-null     uint8\n",
      " 13  Smoking_Status_cigarrettePlus  36 non-null     uint8\n",
      " 14  Smoking_Status_no              36 non-null     uint8\n",
      "dtypes: int64(2), uint8(13)\n",
      "memory usage: 1.3 KB\n"
     ]
    }
   ],
   "source": [
    "# use one hot encoding for categorical variables\n",
    "data = pd.get_dummies(data)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the population since it dominates the predictors  * X\n",
    "scaler = MinMaxScaler()\n",
    "data[\"Population\"] = scaler.fit_transform(pd.DataFrame(data[\"Population\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform a train-test split on the data:\n",
    "- **train data** - 80% of the observations will be used to perfom inference on our model\n",
    "- **test data** - the remaining 20% will be used for testing the correctness of posterior predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths = torch.tensor(data[\"Deaths\"].values, dtype=torch.float)\n",
    "predictors = torch.stack([torch.tensor(data[column].values,dtype=torch.float) \n",
    "                            for column in data.columns if column != \"Deaths\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = torch.Size([28, 14]) \n",
      "y_train.shape = torch.Size([28])\n",
      "\n",
      "X_test.shape = torch.Size([8, 14]) \n",
      "y_test.shape = torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(predictors, deaths, test_size=0.20, \n",
    "                                                    random_state=42,shuffle=True)\n",
    "\n",
    "print(\"X_train.shape =\", X_train.shape,\"\\ny_train.shape =\", y_train.shape)\n",
    "print(\"\\nX_test.shape =\", X_test.shape,\"\\ny_test.shape =\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fit a Poisson bayesian regression model using the number of deaths as the response variable and the other columns as the explanatory variables;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable in our model is the number of deaths and we wish to infer the parameters corresponding to the following predictors\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Deaths}=\\text{exp}(w_0.\\text{Age_Group}+w_1.\\text{Smoking_Status}+w_2.\\text{Population}+b) + \\epsilon\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set a normal prior on $w$, a Log-Normal on the bias term $b$ and a uniformly distributed std for the gaussian noise on $\\hat{y}$\n",
    "\n",
    "\\begin{align*}\n",
    "w&\\sim\\mathcal{N}(0,1)\\\\\n",
    "b&\\sim\\text{LogNormal}(0,1)\\\\\n",
    "\\hat{\\mu}&= \\text{exp}(w x+ b) \\\\\n",
    "y &\\sim \\text{Poisson}(\\hat{\\mu}).\n",
    "\\end{align*}\n",
    "\n",
    "Then we define the family of posterior distributions, by setting a Gamma distribution on $w$ and a Log-Normal on $b$, and run SVI inference on $(x,y)$ data.\n",
    "\n",
    "Notice the prior distribution on the bias term makes this regression problem analytically intractable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 : loss = 217895.35066302758\n",
      "Step 100 : loss = 3383911439229.7007\n",
      "Step 200 : loss = 1411479.4769667494\n",
      "Step 300 : loss = 1118.7773887770516\n",
      "Step 400 : loss = 37302.596219633306\n",
      "Step 500 : loss = 90550.50495592186\n",
      "Step 600 : loss = 3510677.8439829606\n",
      "Step 700 : loss = 108318219136.47739\n",
      "Step 800 : loss = 623256735.4375366\n",
      "Step 900 : loss = 430.7892849275044\n",
      "Step 1000 : loss = 11438910790531.85\n",
      "Step 1100 : loss = 814.2677540949413\n",
      "Step 1200 : loss = 1862.99787238666\n",
      "Step 1300 : loss = 1565471.531037603\n",
      "Step 1400 : loss = 1294.398164476667\n",
      "Step 1500 : loss = 21855.950535433632\n",
      "Step 1600 : loss = 1752.1302978651863\n",
      "Step 1700 : loss = 4.0800620540319803e+24\n",
      "Step 1800 : loss = 2177726.2398586953\n",
      "Step 1900 : loss = 189714.59514348846\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "def death_model(predictors, deaths):\n",
    "    \n",
    "    n_observations, n_predictors = predictors.shape\n",
    "    \n",
    "    # sample weights\n",
    "    w = pyro.sample(\"w\", dist.Normal(torch.zeros(n_predictors), \n",
    "                                        torch.ones(n_predictors)))\n",
    "    b = pyro.sample(\"b\", dist.LogNormal(torch.zeros(1), torch.ones(1)))\n",
    "    \n",
    "    mu_hat = torch.exp((w*predictors).sum(dim=1) + b)\n",
    "    \n",
    "    # condition on the observations\n",
    "    with pyro.plate(\"deaths\", len(deaths)):\n",
    "        pyro.sample(\"obs\", dist.Poisson(mu_hat), obs=deaths)\n",
    "        \n",
    "def death_guide(predictors, deaths=None):\n",
    "    \n",
    "    n_observations, n_predictors = predictors.shape\n",
    "        \n",
    "    w_loc = pyro.param(\"w_loc\", torch.rand(n_predictors), constraint=constraints.positive)\n",
    "    w_scale = pyro.param(\"w_scale\", torch.rand(n_predictors), \n",
    "                         constraint=constraints.positive)\n",
    "    \n",
    "    w = pyro.sample(\"w\", dist.Gamma(w_loc, w_scale))\n",
    "    \n",
    "    b_loc = pyro.param(\"b_loc\", torch.rand(1))\n",
    "    b_scale = pyro.param(\"b_scale\", torch.rand(1), constraint=constraints.positive)\n",
    "    \n",
    "    b = pyro.sample(\"b\", dist.LogNormal(b_loc, b_scale))\n",
    "    \n",
    "\n",
    "# auto guide most of the time gives better result but for learning purpose\n",
    "# manually defined one will be used, there is no big difference\n",
    "# 321-298 improvement\n",
    "\n",
    "#smokeguide = pyro.infer.autoguide.AutoMultivariateNormal(death_model)\n",
    "    \n",
    "death_svi = SVI(model=death_model, guide=death_guide, \n",
    "              optim=optim.ClippedAdam({'lr' : 0.01}), \n",
    "              loss=Trace_ELBO())\n",
    "\n",
    "for step in range(2000):\n",
    "    loss = death_svi.step(X_train, y_train)/len(X_train)\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step} : loss = {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred params: ['w_loc', 'w_scale', 'b_loc', 'b_scale']\n",
      "\n",
      "w_0 = 0.11593594\n",
      "w_1 = 0.10179897\n",
      "w_2 = 0.12004398\n",
      "w_3 = 0.09836756\n",
      "w_4 = 0.18008943\n",
      "w_5 = 0.13515201\n",
      "w_6 = 0.10061953\n",
      "w_7 = 0.09885138\n",
      "w_8 = 0.10401431\n",
      "w_9 = 0.12987916\n",
      "w_10 = 0.80996382\n",
      "w_11 = 1.07109785\n",
      "w_12 = 1.51700020\n",
      "w_13 = 0.91584867\n",
      "b = -11.31038952\n"
     ]
    }
   ],
   "source": [
    "print(\"Inferred params:\", list(pyro.get_param_store().keys()), end=\"\\n\\n\")\n",
    "\n",
    "# w_i and b posterior mean\n",
    "inferred_w = pyro.get_param_store()[\"w_loc\"]\n",
    "inferred_b = pyro.get_param_store()[\"b_loc\"]\n",
    "\n",
    "for i,w in enumerate(inferred_w):\n",
    "    print(f\"w_{i} = {w.item():.8f}\")\n",
    "print(f\"b = {inferred_b.item():.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Posterior predictive distribution**\n",
    "\n",
    "We can use the `Predictive` utility class, corresponding to the posterior predictive distribution, to evaluate our model on test data. Here we compute some summary statistics (mean, std and qualtiles) on $100$ samples from the posterior predictive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled parameter = w\n",
      "\n",
      "        mean       std            5%           50%        95%\n",
      "0   0.000018  0.000040  1.220487e-15  5.197465e-07   0.000088\n",
      "1   0.000007  0.000021  2.174336e-17  5.698389e-08   0.000033\n",
      "2   0.000010  0.000027  1.863642e-15  4.077608e-07   0.000036\n",
      "3   0.000011  0.000031  2.803844e-16  1.145644e-07   0.000076\n",
      "4   0.006052  0.011038  1.042071e-08  9.302986e-04   0.025521\n",
      "5   0.001546  0.004166  1.535496e-13  2.576341e-05   0.008286\n",
      "6   0.000063  0.000256  2.915269e-15  3.726441e-07   0.000314\n",
      "7   0.000036  0.000118  2.506584e-16  2.196494e-07   0.000169\n",
      "8   0.000092  0.000224  3.132012e-15  1.266955e-06   0.000586\n",
      "9   0.000550  0.001281  1.462001e-13  2.881433e-05   0.003084\n",
      "10  5.998237  6.561357  4.735391e-01  3.803491e+00  19.403400\n",
      "11  6.661482  6.679515  5.774164e-01  4.415251e+00  19.460052\n",
      "12  8.580370  6.622905  7.702938e-01  6.812704e+00  21.563299\n",
      "13  6.609722  7.496481  2.560628e-01  4.449547e+00  20.437329\n",
      "\n",
      "Sampled parameter = b\n",
      "\n",
      "       mean       std        5%       50%       95%\n",
      "0  0.000014  0.000006  0.000006  0.000012  0.000025\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print latent params quantile information\n",
    "def summary(samples):\n",
    "    stats = {}\n",
    "    for par_name, values in samples.items():\n",
    "        marginal = pd.DataFrame(values)\n",
    "        percentiles=[.05, 0.5, 0.95]\n",
    "        describe = marginal.describe(percentiles).transpose()\n",
    "        stats[par_name] = describe[[\"mean\", \"std\", \"5%\", \"50%\", \"95%\"]]\n",
    "    return stats\n",
    "\n",
    "# define the posterior predictive\n",
    "predictive = Predictive(model=death_model, guide=death_guide, num_samples=100,\n",
    "                        return_sites=(\"w\",\"b\"))\n",
    "\n",
    "# get posterior samples on test data\n",
    "svi_samples = {k: v.detach().numpy() for k, v in predictive(X_test, y_test).items()}\n",
    "\n",
    "# show summary statistics\n",
    "for key, value in summary(svi_samples).items():\n",
    "    print(f\"Sampled parameter = {key}\\n\\n{value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Evaluate the regression fit on test data using MAE and MSE error metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most known metrics for comparing different regression models are the **Mean Absolute Error** (MAE)\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|$$\n",
    "\n",
    "and the **Mean Squared Error** (MSE)\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2,$$\n",
    "\n",
    "where $n$ is the number of observations, $y$ are the true values `y_test` and $\\hat{y}$ are the predicted values `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE = 321.85369873046875\n",
      "MSE = 168857.0625\n"
     ]
    }
   ],
   "source": [
    "# compute predictions using the inferred paramters\n",
    "y_pred = torch.exp((inferred_w * X_test).sum(1)) + inferred_b\n",
    "\n",
    "print(\"MAE =\", torch.nn.L1Loss()(y_test, y_pred).item())\n",
    "print(\"MSE =\", torch.nn.MSELoss()(y_test, y_pred).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "The Iris dataset contains petal and sepal length and width for three different types of Iris flowers: Setosa, Versicolour, and Virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use logistic regression for multiclass problem, I found two strategies. Belove you can find the results of them.\n",
    "\n",
    "* First one is **converting Bernoulli to Categorical distribution (given hint)** and inferring w,b for each classes. In this method maximum probability is chosen from the result of $e^{w.x + b}$ matrix. \n",
    "\n",
    "Also I tried to use softmax function instead of sigmoid but most probably because of mistake in one of the step I got wrong results. That is why I kept using sigmoid function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the Iris dataset from `sklearn`:\n",
    "```\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "```\n",
    "and perform a train-test split on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "pyro.clear_param_store()\n",
    "\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.infer import Predictive\n",
    "from pyro.infer import autoguide\n",
    "\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pyro.set_rng_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# dataset normalization and OneHotEncoding\n",
    "iris_data = (iris.data - np.min(iris.data))/(np.max(iris.data)-np.min(iris.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = torch.tensor(iris_data,dtype=torch.double)\n",
    "iris_target = torch.tensor(iris.target,dtype=torch.double)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_target, test_size=0.20, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fit a multinomial bayesian logistic regression model on the four predictors petal length/width and sepal length/width. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I used many distributions for guide and `Laplace` gave the best result in terms of overall accuracy and the accuracy among the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 : loss = 1.1749493515175933\n",
      "Step 1000 : loss = 1.205179748685519\n",
      "Step 2000 : loss = 1.1422184678699614\n",
      "Step 3000 : loss = 1.1131413117644433\n",
      "Step 4000 : loss = 1.1625843565338694\n",
      "Step 5000 : loss = 1.1121754668262114\n",
      "Step 6000 : loss = 1.0413399957427572\n",
      "Step 7000 : loss = 1.081022145199147\n",
      "Step 8000 : loss = 1.1231808751010297\n",
      "Step 9000 : loss = 1.0551129989547\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model and guide functions\n",
    "def log_reg_model(x, y):\n",
    "    n_observations, n_predictors = x.shape\n",
    "    \n",
    "    # Here 4x3 w matrix will be learnt\n",
    "    w_zero = pyro.sample(\"w_zero\", dist.Normal(torch.zeros(n_predictors), torch.ones(n_predictors)))\n",
    "    b_zero = pyro.sample(\"b_zero\", dist.Normal(0.,1.))\n",
    "    \n",
    "    w_one = pyro.sample(\"w_one\", dist.Normal(torch.zeros(n_predictors), torch.ones(n_predictors)))\n",
    "    b_one = pyro.sample(\"b_one\", dist.Normal(0.,1.))\n",
    "    \n",
    "    w_two = pyro.sample(\"w_two\", dist.Normal(torch.zeros(n_predictors), torch.ones(n_predictors)))\n",
    "    b_two = pyro.sample(\"b_two\", dist.Normal(0.,1.))\n",
    "    \n",
    "    # non-linearity\n",
    "    yhat_zero = torch.sigmoid((w_zero*x).sum(dim=1) + b_zero)\n",
    "    yhat_one = torch.sigmoid((w_one*x).sum(dim=1) + b_one)\n",
    "    yhat_two = torch.sigmoid((w_two*x).sum(dim=1) + b_two)\n",
    "    \n",
    "    yhat = torch.stack((yhat_zero,yhat_one, yhat_two),dim=1)\n",
    "    \n",
    "    with pyro.plate(\"data\", n_observations):\n",
    "        # sampling 0-1 labels from Bernoulli distribution\n",
    "        y = pyro.sample(\"y\", dist.Categorical(yhat), obs=y)\n",
    "               \n",
    "def log_reg_guide(x, y=None):\n",
    "    \n",
    "    n_observations, n_predictors = x.shape\n",
    "    \n",
    "    w_loc_zero = pyro.param(\"w_loc_zero\", torch.rand(n_predictors))\n",
    "    w_scale_zero = pyro.param(\"w_scale_zero\", torch.rand(n_predictors), constraint=constraints.positive)\n",
    "    w_zero = pyro.sample(\"w_zero\", dist.Laplace(w_loc_zero, w_scale_zero))\n",
    "    \n",
    "    b_loc_zero = pyro.param(\"b_loc_zero\", torch.rand(1))\n",
    "    b_scale_zero = pyro.param(\"b_scale_zero\", torch.rand(1), constraint=constraints.positive)\n",
    "    b_zero = pyro.sample(\"b_zero\", dist.Normal(b_loc_zero, b_scale_zero))\n",
    "    \n",
    "    w_loc_one = pyro.param(\"w_loc_one\", torch.rand(n_predictors))\n",
    "    w_scale_one = pyro.param(\"w_scale_one\", torch.rand(n_predictors), constraint=constraints.positive)\n",
    "    w_one = pyro.sample(\"w_one\", dist.Laplace(w_loc_one, w_scale_one))\n",
    "    \n",
    "    b_loc_one = pyro.param(\"b_loc_one\", torch.rand(1))\n",
    "    b_scale_one = pyro.param(\"b_scale_one\", torch.rand(1), constraint=constraints.positive)\n",
    "    b_one = pyro.sample(\"b_one\", dist.Normal(b_loc_one, b_scale_one))\n",
    "    \n",
    "    w_loc_two = pyro.param(\"w_loc_two\", torch.rand(n_predictors))\n",
    "    w_scale_two = pyro.param(\"w_scale_two\", torch.rand(n_predictors), constraint=constraints.positive)\n",
    "    w_two = pyro.sample(\"w_two\", dist.Laplace(w_loc_two, w_scale_two))\n",
    "    \n",
    "    b_loc_two = pyro.param(\"b_loc_two\", torch.rand(1))\n",
    "    b_scale_two = pyro.param(\"b_scale_two\", torch.rand(1), constraint=constraints.positive)\n",
    "    b_two = pyro.sample(\"b_two\", dist.Normal(b_loc_two, b_scale_two))\n",
    "\n",
    "#auto_guide_log = pyro.infer.autoguide.AutoDelta(log_reg_model)\n",
    "\n",
    "\n",
    "log_reg_svi = SVI(model=log_reg_model, guide=log_reg_guide, \n",
    "          optim=optim.ClippedAdam({'lr' : 0.0002}), \n",
    "          loss=Trace_ELBO()) \n",
    "\n",
    "losses = []\n",
    "for step in range(10000):\n",
    "    loss = log_reg_svi.step(X_train, y_train)/len(X_train)\n",
    "    losses.append(loss)\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step {step} : loss = {loss}\")\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.plot(losses)\n",
    "ax.set_title(\"ELBO loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the inferred parameters for manual \n",
    "param = list(pyro.get_param_store().keys())\n",
    "\n",
    "w_zero = pyro.get_param_store()[param[0]]\n",
    "w_one = pyro.get_param_store()[param[4]]\n",
    "w_two = pyro.get_param_store()[param[8]]\n",
    "b_zero = pyro.get_param_store()[param[2]]\n",
    "b_one = pyro.get_param_store()[param[6]]\n",
    "b_two = pyro.get_param_store()[param[10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction class which returns to output label as 0,1 or 2\n",
    "# by choosing the max probability\n",
    "\n",
    "def predict_class(x):\n",
    "    out_zero = torch.sigmoid((w_zero * x).sum(dim=1) + b_zero)\n",
    "    out_one = torch.sigmoid((w_one * x).sum(dim=1) + b_one)\n",
    "    out_two = torch.sigmoid((w_two * x).sum(dim=1) + b_two)\n",
    "    out = torch.stack((out_zero,out_one,out_two),dim=1)\n",
    "    return torch.argmax(out,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Evaluate your bayesian classifier on test data: compute the overall test accuracy and class-wise accuracy for the three different flower categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_zero = 0\n",
    "count_one = 0\n",
    "count_two = 0\n",
    "for i,j in zip(predict_class(X_test) , y_test):\n",
    "    if j == 0 and i == 0:\n",
    "        count_zero +=1\n",
    "    elif j == 1 and i == 1:\n",
    "        count_one += 1\n",
    "    elif j == 2 and i == 2:\n",
    "        count_two += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class 0 100.00 %\n",
      "Accuracy for class 1 100.00 %\n",
      "Accuracy for class 2 81.82 %\n",
      "Overal test accuracy = 93.33%\n"
     ]
    }
   ],
   "source": [
    "correct_predictions_zero = (count_zero / len(y_test[y_test == 0])) *100\n",
    "correct_predictions_one = (count_one / len(y_test[y_test == 1])) *100\n",
    "correct_predictions_two = (count_two / len(y_test[y_test == 2])) *100\n",
    "\n",
    "correct_predictions_final = (predict_class(X_test) == y_test).sum().item()\n",
    "\n",
    "print(\"Accuracy for class 0 {:.2f}\".format(correct_predictions_zero),\"%\")\n",
    "print(\"Accuracy for class 1 {:.2f}\".format(correct_predictions_one),\"%\")\n",
    "print(\"Accuracy for class 2 {:.2f}\".format(correct_predictions_two),\"%\")\n",
    "\n",
    "print(f\"Overal test accuracy = {correct_predictions_final/len(X_test)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, label 2 is misclassified twice. It is normal because it is hard to classify label 1 and label 2. Overall accuracy and accuracy among the classes are good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Second idea is using, one vs all strategy to use logistic regression for multiclass problem. Here I trained 3 binary classifier and inferred different w and b. I compared the probabilities of three binary classifiers and choose the class which has the highest probability  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the Iris dataset from `sklearn`:\n",
    "```\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "```\n",
    "and perform a train-test split on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "pyro.clear_param_store()\n",
    "\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.infer import Predictive\n",
    "from pyro.infer import autoguide\n",
    "\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pyro.set_rng_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset normalization\n",
    "iris_data = (iris.data - np.min(iris.data))/(np.max(iris.data)-np.min(iris.data))\n",
    "\n",
    "# Each class will be 0 before vs rest will be 1 \n",
    "\n",
    "# 0 vs rest dataset , just make 2 -> 1\n",
    "zerovsrest = np.where((iris.target == 2),1,iris.target)\n",
    "# 1 vs rest dataset , make 1 -> 0 and 0,2 -> 1\n",
    "onevsrest = np.where((iris.target == 2),-1,iris.target)\n",
    "onevsrest = np.where((onevsrest == 0),-1,onevsrest)\n",
    "onevsrest = np.where((onevsrest == 1),0,onevsrest)\n",
    "onevsrest = np.where((onevsrest == -1),1,onevsrest)\n",
    "\n",
    "# 2 vs rest dataset, make 0,1 -> 1 and 2 -> 0\n",
    "twovsrest = np.where((iris.target == 0),1,iris.target)\n",
    "twovsrest = np.where((twovsrest == 2),0,twovsrest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, I prepared the data set for each binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = torch.tensor(iris_data,dtype=torch.double)\n",
    "iris_target = torch.tensor(iris.target,dtype=torch.double)\n",
    "zerovsrest = torch.tensor(zerovsrest,dtype=torch.double)\n",
    "onevsrest = torch.tensor(onevsrest,dtype=torch.double)\n",
    "twovsrest = torch.tensor(twovsrest,dtype=torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fit a multinomial bayesian logistic regression model on the four predictors petal length/width and sepal length/width. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here many distribution in model function and many auto guide were tested like first strategy and best ones chosen which is `LogNormal` for the model and `AutoLaplaceApproximation` for the auto guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and guide functions\n",
    "def log_reg_model(x, y):\n",
    "    n_observations, n_predictors = x.shape\n",
    "    \n",
    "    w = pyro.sample(\"w\", dist.LogNormal(torch.zeros(n_predictors), torch.ones(n_predictors)))\n",
    "    b = pyro.sample(\"b\", dist.LogNormal(0.,1.))\n",
    "    \n",
    "    # non-linearity\n",
    "    yhat = torch.sigmoid((w*x).sum(dim=1) + b)\n",
    "    \n",
    "    with pyro.plate(\"data\", n_observations):\n",
    "        # sampling 0-1 labels from Bernoulli distribution\n",
    "        y = pyro.sample(\"y\", dist.Bernoulli(yhat), obs=y)\n",
    "        \n",
    "        \n",
    "def log_reg_guide(x, y=None):\n",
    "    \n",
    "    n_observations, n_predictors = x.shape\n",
    "    \n",
    "    w_loc = pyro.param(\"w_loc\", torch.rand(n_predictors))\n",
    "    w_scale = pyro.param(\"w_scale\", torch.rand(n_predictors), \n",
    "                         constraint=constraints.positive)\n",
    "    w = pyro.sample(\"w\", dist.Normal(w_loc,w_scale))\n",
    "    \n",
    "    b_loc = pyro.param(\"b_loc\", torch.rand(1))\n",
    "    b_scale = pyro.param(\"b_scale\", torch.rand(1), \n",
    "                         constraint=constraints.positive)\n",
    "    b = pyro.sample(\"b\", dist.Normal(b_loc,b_scale))\n",
    "    \n",
    "auto_guide_log = pyro.infer.autoguide.AutoLaplaceApproximation(log_reg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_infer_parameters(X_train ,y_train, auto = False):\n",
    "    # delete previously inferred params from pyro.param_store()\n",
    "    pyro.clear_param_store()\n",
    "    \n",
    "    log_reg_svi = SVI(model=log_reg_model, guide=auto_guide_log, \n",
    "              optim=optim.ClippedAdam({'lr' : 0.0002}), \n",
    "              loss=Trace_ELBO()) \n",
    "\n",
    "    losses = []\n",
    "    for step in range(10000):\n",
    "        loss = log_reg_svi.step(X_train, y_train)/len(X_train)\n",
    "        losses.append(loss)\n",
    "        if step % 1000 == 0:\n",
    "            print(f\"Step {step} : loss = {loss}\")\n",
    "            \n",
    "    if auto == True:\n",
    "        \n",
    "        param = list(pyro.get_param_store().keys())\n",
    "        inferred = pyro.get_param_store()[param[0]]\n",
    "\n",
    "        inferred_w = inferred[0:len(X_train[0,:])]\n",
    "        inferred_b = inferred[len(X_train[0,:])]\n",
    "        return inferred_w, inferred_b\n",
    "    \n",
    "    else:\n",
    "        w = pyro.get_param_store()[\"w_loc\"]\n",
    "        b = pyro.get_param_store()[\"b_loc\"]\n",
    "\n",
    "        return w,b\n",
    "\n",
    "\n",
    "def return_prob(w,b,x):\n",
    "    out = torch.sigmoid((w * x).sum(dim=1) + b)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 : loss = 0.9493689349039591\n",
      "Step 1000 : loss = 0.8254076303922392\n",
      "Step 2000 : loss = 0.7377631665897586\n",
      "Step 3000 : loss = 0.6744407804802958\n",
      "Step 4000 : loss = 0.6306662108382127\n",
      "Step 5000 : loss = 0.5986701638021614\n",
      "Step 6000 : loss = 0.5727129290434849\n",
      "Step 7000 : loss = 0.551088322621781\n",
      "Step 8000 : loss = 0.5332139734822869\n",
      "Step 9000 : loss = 0.5193220599354917\n",
      "First binary classifier is trained and inferred parameters are stored\n",
      "Step 0 : loss = 1.073270269521865\n",
      "Step 1000 : loss = 0.9734208012655652\n",
      "Step 2000 : loss = 0.9031427923782857\n",
      "Step 3000 : loss = 0.8526560142032932\n",
      "Step 4000 : loss = 0.8160845029062179\n",
      "Step 5000 : loss = 0.78934484221927\n",
      "Step 6000 : loss = 0.769598794583281\n",
      "Step 7000 : loss = 0.7560054581324416\n",
      "Step 8000 : loss = 0.7463864005831833\n",
      "Step 9000 : loss = 0.7389349458949094\n",
      "Second binary classifier is trained and inferred parameters are stored\n",
      "Step 0 : loss = 0.7856684751848356\n",
      "Step 1000 : loss = 0.7710108097064106\n",
      "Step 2000 : loss = 0.7610639386180128\n",
      "Step 3000 : loss = 0.7536580080289761\n",
      "Step 4000 : loss = 0.7479026012891379\n",
      "Step 5000 : loss = 0.7434058879364124\n",
      "Step 6000 : loss = 0.7399345033545729\n",
      "Step 7000 : loss = 0.7373203573376479\n",
      "Step 8000 : loss = 0.7354295237202524\n",
      "Step 9000 : loss = 0.7341448742204117\n",
      "Third binary classifier is trained and inferred parameters are stored\n"
     ]
    }
   ],
   "source": [
    "# Splittig the original dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_target, test_size=0.20, \n",
    "                                                    random_state=42)\n",
    "\n",
    "\n",
    "# split all the dataset , 0->0  vs 1,2 -> 1\n",
    "X_train_zero, X_test_zero, y_train_zero, y_test_zero = train_test_split(iris_data, zerovsrest, test_size=0.20, \n",
    "                                                    random_state=42)\n",
    "w_zero , b_zero = learn_infer_parameters(X_train_zero, y_train_zero,auto=True)\n",
    "\n",
    "out_zero = return_prob(w_zero, b_zero, X_test_zero)\n",
    "print(\"First binary classifier is trained and inferred parameters are stored\")\n",
    "\n",
    "# split all the dataset, 1 -> 0 , 0,2 ->1\n",
    "X_train_one, X_test_one, y_train_one, y_test_one = train_test_split(iris_data, onevsrest, test_size=0.20, \n",
    "                                                    random_state=42)\n",
    "\n",
    "w_one , b_one = learn_infer_parameters(X_train_one, y_train_one,auto=True)\n",
    "\n",
    "out_one = return_prob(w_one, b_one, X_test_one)\n",
    "print(\"Second binary classifier is trained and inferred parameters are stored\")\n",
    "\n",
    "# split all the dataset 2->0 , 0,1 -> 1\n",
    "X_train_two, X_test_two, y_train_two, y_test_two = train_test_split(iris_data, twovsrest, test_size=0.20, \n",
    "                                                    random_state=42)\n",
    "\n",
    "w_two , b_two = learn_infer_parameters(X_train_two, y_train_two,auto=True)\n",
    "\n",
    "out_two = return_prob(w_two, b_two, X_test_two)\n",
    "print(\"Third binary classifier is trained and inferred parameters are stored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Evaluate your bayesian classifier on test data: compute the overall test accuracy and class-wise accuracy for the three different flower categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the predictions basically, each base class labeled as zero so I am choosing the highest probability for \n",
    "# these labels\n",
    "out_final = torch.stack((out_zero,out_one,out_two),dim=1)\n",
    "predictions = torch.argmin(out_final,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2, grad_fn=<SelectBackward>) tensor(1., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(predictions,y_test):\n",
    "    if i != j:\n",
    "        print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_zero = 0\n",
    "count_one = 0\n",
    "count_two = 0\n",
    "for i,j in zip(predictions , y_test):\n",
    "    if j == 0 and i == 0:\n",
    "        count_zero +=1\n",
    "    elif j == 1 and i == 1:\n",
    "        count_one += 1\n",
    "    elif j == 2 and i == 2:\n",
    "        count_two += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class 0 100.00 %\n",
      "Accuracy for class 1 88.89 %\n",
      "Accuracy for class 2 100.00 %\n",
      "Overal test accuracy = 96.67%\n"
     ]
    }
   ],
   "source": [
    "correct_predictions_zero = (count_zero / len(y_test[y_test == 0])) *100\n",
    "correct_predictions_one = (count_one / len(y_test[y_test == 1])) *100\n",
    "correct_predictions_two = (count_two / len(y_test[y_test == 2])) *100\n",
    "\n",
    "correct_predictions_final = (predictions == y_test).sum().item()\n",
    "\n",
    "print(\"Accuracy for class 0 {:.2f}\".format(correct_predictions_zero),\"%\")\n",
    "print(\"Accuracy for class 1 {:.2f}\".format(correct_predictions_one),\"%\")\n",
    "print(\"Accuracy for class 2 {:.2f}\".format(correct_predictions_two),\"%\")\n",
    "\n",
    "print(f\"Overal test accuracy = {correct_predictions_final/len(X_test)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there is only one misclassification and overall accuracy is good and the accuracy among the classes are good as well. Having misclassification is normal since it is hard to classify 1 and 2. Indeed having only one classification error is satisfying."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
