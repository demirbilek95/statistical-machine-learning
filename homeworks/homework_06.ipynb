{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Poisson regression is a Generalized Linear Model, used to model count data. It takes the form\n",
    "\n",
    "$$\\mathbb{E}(\\mu|x)=\\exp(w_1\\,x_1+\\ldots+w_k\\,x_k+b),$$\n",
    "\n",
    "where the observed counts $y$ are drawn from a Poisson distribution on the expected counts: \n",
    "\n",
    "$$y_i \\sim \\text{Poisson}(\\mu_i).$$\n",
    "\n",
    "1. Download and import Load the smoking dataset from: [https://data.princeton.edu/wws509/datasets/#smoking](https://data.princeton.edu/wws509/datasets/#smoking). Then perform a train-test split on the data;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.infer import Predictive\n",
    "\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "\n",
    "pyro.set_rng_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age_Group</th>\n",
       "      <th>Smoking_Status</th>\n",
       "      <th>Population</th>\n",
       "      <th>Deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>656</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>359</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>249</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>632</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1067</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age_Group  Smoking_Status  Population  Deaths\n",
       "0          1               1         656      18\n",
       "1          2               1         359      22\n",
       "2          3               1         249      19\n",
       "3          4               1         632      55\n",
       "4          5               1        1067     117"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"https://data.princeton.edu/wws509/datasets/smoking.raw\",sep=\"\\t\",header=None)\n",
    "data.columns = [\"Age_Group\",\"Smoking_Status\",\"Population\",\"Deaths\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* age at the start of follow-up: in five-year age groups coded 1 to 9 for 40-44, 45-49, 50-54, 55-59, 60-64, 65-69, 70-74, 75-79, 80+.\n",
    "* smoking status: coded 1 = never smoked, 2 = smoked cigars or pipe only, 3 = smoked cigarettes and cigar or pipe, and 4 = smoked cigarettes only,\n",
    "* population: number of male pensioners followed, and\n",
    "* deaths: number of deaths in a six-year period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36 entries, 0 to 35\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype\n",
      "---  ------          --------------  -----\n",
      " 0   Age_Group       36 non-null     int64\n",
      " 1   Smoking_Status  36 non-null     int64\n",
      " 2   Population      36 non-null     int64\n",
      " 3   Deaths          36 non-null     int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 1.2 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform a train-test split on the data:\n",
    "- **train data** - 80% of the observations will be used to perfom inference on our model\n",
    "- **test data** - the remaining 20% will be used for testing the correctness of posterior predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data[\"Population\"] = scaler.fit_transform(pd.DataFrame(data[\"Population\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = torch.Size([28, 3]) \n",
      "y_train.shape = torch.Size([28])\n",
      "\n",
      "X_test.shape = torch.Size([8, 3]) \n",
      "y_test.shape = torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "deaths = torch.tensor(data[\"Deaths\"].values, dtype=torch.float)\n",
    "predictors = torch.stack([torch.tensor(data[column].values,dtype=torch.float) \n",
    "                            for column in data.columns if column != \"Deaths\"],1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors, deaths, test_size=0.20, \n",
    "                                                    random_state=42,shuffle=True)\n",
    "\n",
    "print(\"X_train.shape =\", X_train.shape,\"\\ny_train.shape =\", y_train.shape)\n",
    "print(\"\\nX_test.shape =\", X_test.shape,\"\\ny_test.shape =\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fit a Poisson bayesian regression model using the number of deaths as the response variable and the other columns as the explanatory variables;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable in our model is the number of deaths and we wish to infer the parameters corresponding to the following predictors\n",
    "\n",
    "$$\n",
    "\\text{Deaths}=\\text{exp}(w_0\\cdot\\text{Age_Group}+w_1\\cdot\\text{Smoking_Status}+w_2\\cdot\\text{Population}+b) + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set a normal prior on $w$, a Log-Normal on the bias term $b$ and a uniformly distributed std for the gaussian noise on $\\hat{y}$\n",
    "\n",
    "\\begin{align*}\n",
    "w&\\sim\\mathcal{N}(0,1)\\\\\n",
    "b&\\sim\\text{LogNormal}(0,1)\\\\\n",
    "\\hat{\\mu}&= \\text{exp}(w x+ b) \\\\\n",
    "y &\\sim \\text{Poisson}(\\hat{\\mu}).\n",
    "\\end{align*}\n",
    "\n",
    "Then we define the family of posterior distributions, by setting a Gamma distribution on $w$ and a Log-Normal on $b$, and run SVI inference on $(x,y)$ data.\n",
    "\n",
    "Notice the prior distribution on the bias term makes this regression problem analytically intractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**will be read**\n",
    "\n",
    "https://towardsdatascience.com/an-illustrated-guide-to-the-poisson-regression-model-50cccba15958\n",
    "\n",
    "https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Poisson_Regression.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Poisson ile ilgili okuma yapılacak, tanım aralıklarına bakılacak**\n",
    "\n",
    "**w  için negatif değer bulunmaması gerekiyor ona bakılacak, muhati nasıl pozitif yaparım** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_observations, n_predictors = predictors.shape\n",
    "\n",
    "# sample weights\n",
    "w = pyro.sample(\"w\", dist.Normal(torch.zeros(n_predictors), \n",
    "                                    torch.ones(n_predictors)))\n",
    "b = pyro.sample(\"b\", dist.LogNormal(torch.zeros(1), torch.ones(1)))\n",
    "\n",
    "mu_hat = torch.exp((w*predictors).sum(dim=1) + b)\n",
    "\n",
    "# condition on the observations\n",
    "with pyro.plate(\"deaths\", len(deaths)):\n",
    "    pyro.sample(\"obs\", dist.Poisson(mu_hat), obs=deaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 : loss = 93.95904425425189\n",
      "Step 100 : loss = 2803.019447884389\n",
      "Step 200 : loss = 1522.4704756853837\n",
      "Step 300 : loss = 6301.159454096109\n",
      "Step 400 : loss = 401.5093303886907\n",
      "Step 500 : loss = 260685.97967164963\n",
      "Step 600 : loss = 3393509.984654016\n",
      "Step 700 : loss = 348.2024760799749\n",
      "Step 800 : loss = 361426.7742816306\n",
      "Step 900 : loss = 71.80504192199025\n",
      "Step 1000 : loss = 54017015.67062563\n",
      "Step 1100 : loss = 438.7351415157318\n",
      "Step 1200 : loss = 622.0738740733692\n",
      "Step 1300 : loss = 41085367.382489525\n",
      "Step 1400 : loss = 55.8839810905712\n",
      "Step 1500 : loss = 828.5340937290873\n",
      "Step 1600 : loss = 450.55361250681534\n",
      "Step 1700 : loss = 476065.80741048924\n",
      "Step 1800 : loss = 735.5033743626306\n",
      "Step 1900 : loss = 23374.32838944878\n"
     ]
    }
   ],
   "source": [
    "def death_model(predictors, deaths):\n",
    "    \n",
    "    n_observations, n_predictors = predictors.shape\n",
    "    \n",
    "    # sample weights\n",
    "    w = pyro.sample(\"w\", dist.Normal(torch.zeros(n_predictors), \n",
    "                                        torch.ones(n_predictors)))\n",
    "    b = pyro.sample(\"b\", dist.LogNormal(torch.zeros(1), torch.ones(1)))\n",
    "    \n",
    "    mu_hat = torch.exp((w*predictors).sum(dim=1) + b)\n",
    "    \n",
    "    # condition on the observations\n",
    "    with pyro.plate(\"deaths\", len(deaths)):\n",
    "        pyro.sample(\"obs\", dist.Poisson(mu_hat), obs=deaths)\n",
    "        \n",
    "def death_guide(predictors, deaths=None):\n",
    "    \n",
    "    n_observations, n_predictors = predictors.shape\n",
    "        \n",
    "    w_loc = pyro.param(\"w_loc\", torch.rand(n_predictors), constraint=constraints.positive)\n",
    "    w_scale = pyro.param(\"w_scale\", torch.rand(n_predictors), \n",
    "                         constraint=constraints.positive)\n",
    "    \n",
    "    w = pyro.sample(\"w\", dist.Gamma(w_loc, w_scale))\n",
    "    \n",
    "    b_loc = pyro.param(\"b_loc\", torch.rand(1))\n",
    "    b_scale = pyro.param(\"b_scale\", torch.rand(1), constraint=constraints.positive)\n",
    "    \n",
    "    b = pyro.sample(\"b\", dist.LogNormal(b_loc, b_scale))\n",
    "    \n",
    "death_svi = SVI(model=death_model, guide=death_guide, \n",
    "              optim=optim.ClippedAdam({'lr' : 0.01}), \n",
    "              loss=Trace_ELBO())\n",
    "\n",
    "for step in range(2000):\n",
    "    loss = death_svi.step(X_train, y_train)/len(X_train)\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step} : loss = {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inferred params:\", list(pyro.get_param_store().keys()), end=\"\\n\\n\")\n",
    "\n",
    "# w_i and b posterior mean\n",
    "inferred_w = pyro.get_param_store()[\"w_loc\"]\n",
    "inferred_b = pyro.get_param_store()[\"b_loc\"]\n",
    "\n",
    "for i,w in enumerate(inferred_w):\n",
    "    print(f\"w_{i} = {w.item():.8f}\")\n",
    "print(f\"b = {inferred_b.item():.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Posterior predictive distribution**\n",
    "\n",
    "We can use the `Predictive` utility class, corresponding to the posterior predictive distribution, to evaluate our model on test data. Here we compute some summary statistics (mean, std and qualtiles) on $100$ samples from the posterior predictive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled parameter = w\n",
      "\n",
      "       mean       std        5%       50%       95%\n",
      "0  0.651493  0.761568  0.005992  0.398287  2.404526\n",
      "1  0.152532  0.158151  0.000753  0.095998  0.491336\n",
      "2  3.787127  2.943419  0.568067  2.898957  9.268196\n",
      "\n",
      "Sampled parameter = b\n",
      "\n",
      "      mean       std        5%      50%       95%\n",
      "0  0.94716  0.250861  0.602919  0.90823  1.464611\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print latent params quantile information\n",
    "def summary(samples):\n",
    "    stats = {}\n",
    "    for par_name, values in samples.items():\n",
    "        marginal = pd.DataFrame(values)\n",
    "        percentiles=[.05, 0.5, 0.95]\n",
    "        describe = marginal.describe(percentiles).transpose()\n",
    "        stats[par_name] = describe[[\"mean\", \"std\", \"5%\", \"50%\", \"95%\"]]\n",
    "    return stats\n",
    "\n",
    "# define the posterior predictive\n",
    "predictive = Predictive(model=death_model, guide=death_guide, num_samples=100,\n",
    "                        return_sites=(\"w\",\"b\"))\n",
    "\n",
    "# get posterior samples on test data\n",
    "svi_samples = {k: v.detach().numpy() for k, v in predictive(X_test, y_test).items()}\n",
    "\n",
    "# show summary statistics\n",
    "for key, value in summary(svi_samples).items():\n",
    "    print(f\"Sampled parameter = {key}\\n\\n{value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Evaluate the regression fit on test data using MAE and MSE error metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most known metrics for comparing different regression models are the **Mean Absolute Error** (MAE)\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|$$\n",
    "\n",
    "and the **Mean Squared Error** (MSE)\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2,$$\n",
    "\n",
    "where $n$ is the number of observations, $y$ are the true values `y_test` and $\\hat{y}$ are the predicted values `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE = 715.705810546875\n",
      "MSE = 1427406.625\n"
     ]
    }
   ],
   "source": [
    "# compute predictions using the inferred paramters\n",
    "y_pred = torch.exp((inferred_w * X_test).sum(1)) + inferred_b\n",
    "\n",
    "print(\"MAE =\", torch.nn.L1Loss()(y_test, y_pred).item())\n",
    "print(\"MSE =\", torch.nn.MSELoss()(y_test, y_pred).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "The Iris dataset contains petal and sepal length and width for three different types of Iris flowers: Setosa, Versicolour, and Virginica.\n",
    "\n",
    "1. Import the Iris dataset from `sklearn`:\n",
    "```\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "```\n",
    "and perform a train-test split on the data.\n",
    "\n",
    "2. Fit a multinomial bayesian logistic regression model on the four predictors petal length/width and sepal length/width. \n",
    "\n",
    "3. Evaluate your bayesian classifier on test data: compute the overall test accuracy and class-wise accuracy for the three different flower categories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
